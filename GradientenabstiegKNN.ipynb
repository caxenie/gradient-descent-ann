{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h1><center>Einsatz von Gradientenabstiegsverfahren in Neuronalen Netzen</center></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inhalt\n",
    "\n",
    "- Grundlagen der Optimierung\n",
    "- Gradientenabstieg\n",
    "- Lernen als Optimierung in Neuronalen Netzen\n",
    "- Gradientenabstieg in Neuronalen Netzen\n",
    "- Fazit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Grundlagen der Optimierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "**Optimierungsalgorithmen** sind in der Regel **iterative Verfahren**. \n",
    "\n",
    "Ausgehend von einem gegebenen Punkt $ {x_0 \\color{red}{+}} $ erzeugen sie eine Folge ${x_k}$ von **Iterierten**, die zu einer Lösung ($\\color{red}{\\bullet}$) **konvergieren** [2].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![](./optimization.gif)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Grundlagen der Optimierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimierung nullter Ordnung\n",
    "<center>![](./backprop-sketch/opt-types1-notext.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimierung erster Ordnung\n",
    "<center>![](./backprop-sketch/opt-types2-notext-single.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimierung zweiter Ordnung\n",
    "<center>![](./backprop-sketch/opt-types3-notext-single.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradientenabstieg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/gradient-descent-fig.png)</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lernen als Optimierung in Neuronale Netzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Optimierungsalgorithmen helfen uns, eine **Zielfunktion zu minimieren (oder zu maximieren)**. Ein solche Zielfunktion ist in **neuronalen Netzen** eine **mathematische Funktion (E)**, die von den **internen lernbaren Parametern (W)** des Netzwerks abhängt [3].\n",
    "\n",
    "Diese **Parametern** werden bei der Berechnung der **erwarteten Werte (Y)** aus dem Satz von **Prädiktoren (X)** benutzt. \n",
    "\n",
    "**E** beschreibt die Differenz zwischen dem erwarteten Wert und dem **tatsächlichen Netzwerkausgangswert (O)** [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/basic-net.gif)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradientenabstieg in Neuronale Netzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Binäre Klassifikation mit neuronalen Netzen\n",
    "\n",
    "<center>![](./backprop-sketch/hausaufgabe.png)</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neuronale Netzwerkstruktur\n",
    "\n",
    "Neuronales Netzwerk zur Lösung der binären Klassifizierungsaufgabe.\n",
    "\n",
    "Neuronen in verknüpften Schichten sind über Kanten mit Gewichten $wij$ verbunden (d.h. welche sind die Netzwerkparameter). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/1.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aktivierungsfunktion \n",
    "\n",
    "Jedes Neuron hat eine Gesamteingabe $\\color{blue}{x}$ , eine Aktivierungsfunktion $f(\\color{blue}{x})$ und eine Ausgabe $ \\color{red}{y}=f(\\color{blue}{x})$\n",
    "\n",
    "$f(\\color{blue}{x})$ muss eine nichtlineare Funktion sein (z. B. ReLu, tanh, sigmoid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/2.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fehler- / Verlustfunktion\n",
    "\n",
    "Das Netzwerk lernt die Gewichte, so dass für alle Eingänge $ \\color{blue}{x_{input}}$ die vorhergesagte Ausgabe $\\color{red}{y_{output}}$ nah an $\\color{red}{y_{target}}$ ist. \n",
    "\n",
    "Fehlerfunktion ist die mittlere quadratische Fehler:\n",
    "$ E(\\color{red}{y_{output},y_{target}})=\\frac{1}{2}(\\color{red}{y_{output}-y_{target}})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/3.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forwardpropagation\n",
    "\n",
    "Das Netzwerk verwendet Eingabebeispiele $(\\color{blue}{x_{input}},\\color{red}{y_{target}})$, um alle Neuronen zu aktualisieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/4.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forwardpropagation\n",
    "\n",
    "Um die verborgenen Schichten zu aktualisieren, verwendet das Netzwerk die Ausgabe $\\color{red}{y}$ der vorherigen Schicht und berechnet mit den Gewichten die Eingabe $\\color{blue}{x}$ der Neuronen in der nächsten Schicht:\n",
    "\n",
    "$\\color{blue}{x_j} = \\sum_{i\\in in(j)} w_{ij}\\color{red} {y_i} +b_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/5.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forwardpropagation\n",
    "\n",
    "Das Netzwerk aktualisiert die Ausgabe der Neuronen in den verborgenen Schichten durch die nichtlineare Aktivierungsfunktion, $f(\\color{blue}{x})$.\n",
    "\n",
    "$\\color{red}{y}=f(\\color{blue}{x}) = tanh(\\color{blue}{x}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/6.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forwardpropagation\n",
    "\n",
    "Jedes Neuron im Netzwerk verbreitet die Eingabe im Rest des Netzwerks, um die Ausgabe zu berechnen:\n",
    "\n",
    "$\\color{red}{y}=f(\\color{blue}{x}) = tanh(\\color{blue}{x}) $\n",
    "\n",
    "$\\color{blue}{x_j} = \\sum_{i\\in in(j)} w_{ij}\\color{red} {y_i} +b_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/7.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradientenabstieg\n",
    "\n",
    "Der **Backpropagation-Algorithmus** berechnet die Aktualisierungsmenge der Gewichte basierend auf der Änderung der Fehlerfunktion in Bezug auf jedes Gewicht $\\color{magenta}{\\frac{dE}{dw_{ij}}}$. \n",
    "\n",
    "Die Gewichtsaktualisierung folgt dem **Gradientenabstieg**: \n",
    "$w_{ij} = w_{ij} - \\alpha\\color{magenta}{\\frac{dE}{dw_{ij}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/8.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradientenabstieg\n",
    "\n",
    "Um $\\color{magenta}{\\frac{dE}{dw_{ij}}}$ zu berechnen, müssen wir ermitteln, wie der Fehler ändert sich in Abhängigkeit von:\n",
    "- dem Gesamteingang des Neurons  $\\color{orange}{\\frac{dE}{dx}}$ und\n",
    "- der Ausgabe des Neurons  $\\color{green}{\\frac{dE}{dy}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/9.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradientenabstieg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradientenabstieg\n",
    "\n",
    "Die Ableitung des Fehlers für unseren Klassifikator\n",
    "\n",
    "$ E(\\color{red}{y_{output},y_{target}})=\\frac{1}{2}(\\color{red}{y_{output}-y_{target}})^2$\n",
    "\n",
    "ist \n",
    "\n",
    "$\\color{green}{\\frac{\\partial{E}}{\\partial{y_{output}}}} = \\color{red}{y_{output}-y_{target}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/10.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "Da nun $\\color{green}{\\frac{dE}{dy}}$ verfügbar ist, kann $\\color{orange}{\\frac{dE}{dx}}$ mit der Kettenregel aus der vorigen Ebene errechnet werden:\n",
    "\n",
    "$\\color{orange}{\\frac{dE}{dx}} = \\color{green}{\\frac{dE}{dy}}\\frac{dy}{dx} = \\color{green}{\\frac{dE}{dy}}\\frac{df(\\color{blue}{x})}{dx} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/11.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "Sobald wir die Fehlerableitung bezüglich der Eingabe eines Neurons haben, können wir die Fehlerableitung bezüglich der in dieses Neuron führenden Gewichte erhalten:\n",
    "\n",
    "$\\color{magenta}{\\frac{\\partial{E}}{\\partial{w_{ij}}}} = \\color{orange}{\\frac{\\partial{E}}{\\partial{x_j}}} \\frac{\\partial{x_j}}{\\partial{w_{ij}}} = \\color{orange}{\\frac{\\partial{E}}{\\partial{x_j}}} \\color{red}{y_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/12.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "Ebenso kann $\\color{green}{\\frac{dE}{dy}} $ mit der Kettenregel aus der vorigen Ebene errechnet werden:\n",
    "\n",
    "$\\color{green}{\\frac{\\partial{E}}{\\partial{y_{i}}}} = \\sum_{j\\in out(i)} \\color{orange}{\\frac{\\partial{E}}{\\partial{x_j}}} \\frac{\\partial{x_j}}{\\partial{y_{i}}} = \\sum_{j\\in out(i)} \\color{orange}{\\frac{\\partial{E}}{\\partial{x_j}}} w_{ij} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/13.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "Das Netzwerk wiederholt die Schritte für jedes Eingabebeispiel für eine bestimmte Anzahl von Epochen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/14.gif)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_style": "center",
    "code_folding": [
     10,
     14,
     24,
     50,
     69,
     124,
     199
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x164709feb38>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN AUC=1.000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1646fccda90>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Einfaches neuronales Netz zur binären Klassifikation\n",
    "import random\n",
    "import math\n",
    "import time as t\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def f(x):\n",
    "    return math.tanh(x)\n",
    "\n",
    "\n",
    "def df(x):\n",
    "    try:\n",
    "        return 1 / (math.cosh(x) ** 2)\n",
    "    except OverflowError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "class Network:\n",
    "    \n",
    "\n",
    "    def __init__(self, l_in, l_hid, l_out, rate=0.05):\n",
    "        self.num_layer_in = l_in\n",
    "        self.num_layer_hid = l_hid\n",
    "        self.num_layer_out = l_out\n",
    "\n",
    "        self.w1 = []\n",
    "        self.w2 = []\n",
    "\n",
    "        # init weights inputs -> hidden\n",
    "        for i in range(self.num_layer_in * self.num_layer_hid ):\n",
    "            self.w1.append(random.randrange(-10, 10) * 0.01)\n",
    "\n",
    "        # init weights hidden -> output\n",
    "        for i in range(self.num_layer_hid * self.num_layer_out):\n",
    "            self.w2.append(random.randrange(-10, 10) * 0.01)\n",
    "\n",
    "        # init weights bias -> hidden\n",
    "        for i in range(self.num_layer_hid):\n",
    "            self.w1.append(random.randrange(-10, 10) * 0.01)\n",
    "\n",
    "        # init weights bias -> output\n",
    "        for i in range(self.num_layer_out):\n",
    "            self.w2.append(random.randrange(-10, 10) * 0.01)\n",
    "\n",
    "        self.learning_rate = rate\n",
    "\n",
    "    def forward_propagation(self, data_in):\n",
    "        net_hid = 0\n",
    "        out_hid = []\n",
    "        net_out = 0\n",
    "\n",
    "        # Calculate net input and outputs for hidden layer\n",
    "        for i in range(self.num_layer_hid):\n",
    "            net_hid += float(data_in[0]) * self.w1[i]\n",
    "            net_hid += float(data_in[1]) * self.w1[i + self.num_layer_hid]\n",
    "            net_hid += self.w1[i + self.num_layer_in * self.num_layer_hid]\n",
    "            out_hid.append(f(net_hid))\n",
    "\n",
    "        # Calculate net input and output for output layer\n",
    "        for i in range(self.num_layer_hid):\n",
    "            net_out += out_hid[i] * self.w2[i]\n",
    "        net_out += self.w2[self.num_layer_hid * self.num_layer_out]\n",
    "\n",
    "        return f(net_out)\n",
    "\n",
    "    def train(self, train_data):\n",
    "        for data in train_data:\n",
    "            net_hid = []\n",
    "            out_hid = []\n",
    "            net_out = 0\n",
    "            out_out = 0\n",
    "\n",
    "            updates_w1 = []\n",
    "            updates_w2 = []\n",
    "\n",
    "            # Calculate net input and outputs for hidden layer\n",
    "            net = 0\n",
    "            for i in range(self.num_layer_hid):\n",
    "                net += float(data[0]) * self.w1[i]\n",
    "                net += float(data[1]) * self.w1[i + self.num_layer_hid]\n",
    "                net += self.w1[i + self.num_layer_in * self.num_layer_hid]\n",
    "                net_hid.append(net)\n",
    "                out_hid.append(f(net))\n",
    "\n",
    "            # Calculate net input and output for output layer\n",
    "            net = 0\n",
    "            for i in range(self.num_layer_hid):\n",
    "                net += out_hid[i] * self.w2[i]\n",
    "            net += self.w2[self.num_layer_hid * self.num_layer_out]\n",
    "            net_out = net\n",
    "            out_out = f(net)\n",
    "\n",
    "            # w2 weights update\n",
    "            for i in range(len(self.w2)):\n",
    "                if i < len(self.w2) - self.num_layer_out:\n",
    "                    update = self.learning_rate * (float(data[2]) - out_out) * df(net_out) * out_hid[i]\n",
    "                else:\n",
    "                    update = self.learning_rate * (float(data[2]) - out_out) * df(net_out)\n",
    "                updates_w2.append(update)\n",
    "\n",
    "            # w1 weights update\n",
    "            for i in range(len(self.w1)):\n",
    "                if i < len(self.w1) - self.num_layer_hid:\n",
    "                    if i < 4:\n",
    "                        update = self.learning_rate * (float(data[2]) - out_out) * df(net_out) * self.w2[i % 4] * df(net_hid[i % 4]) * float(data[0])\n",
    "                    else:\n",
    "                        update = self.learning_rate * (float(data[2]) - out_out) * df(net_out) * self.w2[i % 4] * df(net_hid[i % 4]) * float(data[1])\n",
    "                else:\n",
    "                    update = self.learning_rate * (float(data[2]) - out_out) * df(net_out) * self.w2[i % 4] * df(net_hid[i % 4])\n",
    "                updates_w1.append(update)\n",
    "\n",
    "            # update weights w1\n",
    "            for i, update in enumerate(updates_w1):\n",
    "                self.w1[i] += update\n",
    "\n",
    "            # update weights w2\n",
    "            for i, update in enumerate(updates_w2):\n",
    "                self.w2[i] += update\n",
    "\n",
    "\n",
    "def main():\n",
    "    network = Network(2, 5, 1)\n",
    "    train_data = []\n",
    "    f = open('./input_dataset.in')\n",
    "\n",
    "    try:\n",
    "\n",
    "        while 1: \n",
    "            new_in = f.readline().split(',')\n",
    "            if len(new_in) < 3:\n",
    "                break\n",
    "            train_data.append(new_in)\n",
    "            \n",
    "\n",
    "        for _ in range(100):\n",
    "            network.train(train_data)\n",
    "\n",
    "        f = open('./testing_dataset.in')\n",
    "        out_data = []\n",
    "        test_data = []\n",
    "        out_data_prob = []\n",
    "        while 1:\n",
    "            new_in = f.readline().split(',')\n",
    "            if len(new_in) < 2:\n",
    "                break\n",
    "            new_in[0] = new_in[0][new_in[0].rfind(' ') + 1:]\n",
    "            new_in[1] = new_in[1][new_in[1].rfind(' ') + 1:]\n",
    "            output = network.forward_propagation(new_in)\n",
    "            test_data.append(new_in)\n",
    "            out_data_prob.append(output)\n",
    "            if output > 0:\n",
    "                output = '+1'\n",
    "            else:\n",
    "                output = '-1'\n",
    "            out_data.append(output)\n",
    "                \n",
    "        f = open('./expected_dataset.in')\n",
    "        out_expect_data = []\n",
    "        while 1:\n",
    "            new_in = f.readline().split('\\n')\n",
    "            if len(new_in) < 2:\n",
    "                break\n",
    "            out_expect_data.append(new_in[0])    \n",
    "\n",
    "    except EOFError:\n",
    "        print('', end='')\n",
    "        \n",
    "    plt.figure(3)\n",
    "    plt.plot(out_data, 'bo') \n",
    "    plt.plot(out_expect_data,'r+')\n",
    "    plt.title('Klassenlabels')\n",
    "    plt.box(\"off\")\n",
    "    plt.savefig('output-klass.png')\n",
    "    plt.show()\n",
    "\n",
    "    # AUC / ROC curves \n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from matplotlib import pyplot\n",
    "    lr_auc = roc_auc_score(list(map(int, out_expect_data)), out_data_prob)\n",
    "    # summarize scores\n",
    "    print('NN AUC=%.3f' % (lr_auc))\n",
    "    # calculate roc curves\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(list(map(int, out_expect_data)), out_data_prob)\n",
    "    # plot the roc curve for the model\n",
    "    pyplot.plot(lr_fpr, lr_tpr, marker='.')\n",
    "    # axis labels\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.box(\"off\")\n",
    "    # show the legend\n",
    "    pyplot.savefig('output-roc.png')\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Analyse des Gradientenabstiegs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Abhängig von der Datenmenge machen wir einen **Kompromiss zwischen die Genauigkeit** und der **Konvergenzgeschwindigkeit** [2, 4]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Analyse des Gradientenabstiegs : Genauigkeit\n",
    "\n",
    "<center>![](./precision.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Analyse des Gradientenabstiegs : Konvergenzgeschwindigkeit\n",
    "\n",
    "<center>![](./gradient-problems.gif)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Analyse des Gradientenabstiegs : Konvergenzgeschwindigkeit\n",
    "<center>![](./demo-anim.gif)</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fazit\n",
    "\n",
    "- **Gradientenabstieg** ist die **typische Optimierungsmethode** in neuronalen Netzen.\n",
    "- **Lernen** in Neuronalen Netzen ist ein **iterativer Prozess**.\n",
    "- Bei der **Backpropagation** wird ein Gradientenabstieg verwendet, um zu einer **Lösung zu konvergieren** (d. h. die **Gewichte zu finden**), die die **Fehlerfunktion minimiert**.\n",
    "- **Gradientenabstieg** ist jedoch **problematisch** (Konvergenz, Präzision), aber es wurden viele **verbesserte Verfahren** entwickelt.\n",
    "- Das **Verständnis des Gradientenabstieg** bei der Diagnose der Backpropagation **ist für erfolgreiche Anwendungen erforderlich**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Literaturverzeichnis\n",
    "\n",
    "[1] https://google-developers.appspot.com/machine-learning/ (letzter Besuch, Dez 2019)\n",
    "\n",
    "[2] Boyd, S., & Vandenberghe, L. (2004). Convex optimization. Cambridge university press.\n",
    "\n",
    "[3] Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press.\n",
    "\n",
    "[4] Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vorlesung Notebook herunterladen\n",
    "\n",
    "<img src=\"./qrcode.png\" style=\"width = 200, height=200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Nachteile des Gradientenabstiegs\n",
    "\n",
    "Es wurden **verschiedene Methoden entwickelt**, um die Nechteile zum **beheben** [3]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "| Gradient Descent | Nesterov Momentum |  Adaptive Moment Estimation (Adam) | \n",
    "|:---------:|:---------:|:---------:|\n",
    "| $ w_{t+1} = w_t - {\\alpha} {\\nabla E(w_t)}$   |  $ w_{t+1} = w_t - {\\alpha} {v_t} \\\\ { v_{t+1} = \\frac{\\gamma}{\\alpha} v_t  + {\\nabla E(w_t - \\gamma v_t)}} $  | $ w_{t+1} = w_t -{\\alpha} {\\frac{m_t}{\\sqrt{{v_t}}}}  \\\\ {m_{t+1} = f(\\nabla E(w_t) ) \\\\ v_{t+1} = f(\\nabla E(w_t)^2) }$| \n",
    "| $\\color{green}{+} $ einfache Aktualisierungsregel (z. B. Summe, Produkt)  | $\\color{green}{+}$ Beschleunigt in die jeweilige Steigungsrichtung und dämpft Schwingungen | $\\color{green}{+}$ speichert einen exponentiell abfallenden Durchschnitt vergangener Gradienten (glättendes Moment) mit adaptiver Lernrate für jeden parameter |\n",
    "| $\\color{red} {-}$ Konvergiert zum globalen Minimum für konvexe Fehleroberflächen und zu einem lokalen Minimum für nichtkonvexe Oberflächen | $\\color{red}{-}$ funktioniert nicht gut, wenn die Kostenfunktionen stark konvex sind | $\\color{red}{-}$ hat den niedrigsten Trainingsfehler, aber nicht den niedrigsten Validierungsfehler, und der Validierungsfehler ist größer als der Trainingsfehler (d. h. leichte Überanpassung) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Analyse\n",
    "<center>![](./loss-anim.gif)</center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
