{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Einsatz von Gradientenabstiegsverfahren in Neuronalen Netzen</center></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inhalt\n",
    "\n",
    "- Grundlagen der Optimierung\n",
    "- Gradientenabstieg\n",
    "- Lernen als Optimierung in Neuronalen Netzen\n",
    "- Gradientenabstieg in Neuronalen Netzen\n",
    "- Fazit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Grundlagen der Optimierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "**Optimierungsalgorithmen** sind in der Regel **iterative Verfahren**. \n",
    "\n",
    "Ausgehend von einem gegebenen Punkt $ {x_0 \\color{red}{+}} $ erzeugen sie eine Folge ${x_k}$ von **Iterierten**, die zu einer Lösung ($\\color{red}{\\bullet}$) **konvergieren** [2].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![](./optimization.gif)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Grundlagen der Optimierung\n",
    "### Optimierung nullter Ordnung\n",
    "<center>![](./backprop-sketch/opt-types1-notext.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Grundlagen der Optimierung\n",
    "### Optimierung erster Ordnung\n",
    "<center>![](./backprop-sketch/opt-types2-notext-single.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Grundlagen der Optimierung\n",
    "### Optimierung zweiter Ordnung\n",
    "<center>![](./backprop-sketch/opt-types3-notext-single.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradientenabstieg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/gradient-descent-fig.png)</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lernen als Optimierung in Neuronale Netzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Optimierungsalgorithmen helfen uns, eine **Zielfunktion zu minimieren (oder zu maximieren)**. Ein solche Zielfunktion ist in **neuronalen Netzen** eine **mathematische Funktion (E)**, die von den **internen lernbaren Parametern (W)** des Netzwerks abhängt [3].\n",
    "\n",
    "Diese **Parametern** werden bei der Berechnung der **erwarteten Werte (Y)** aus dem Satz von **Prädiktoren (X)** benutzt. \n",
    "\n",
    "**E** beschreibt die Differenz zwischen dem erwarteten Wert und dem **tatsächlichen Netzwerkausgangswert (O)** [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/basic-net.gif)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradientenabstieg in Neuronale Netzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Binäre Klassifikation mit neuronalen Netzen\n",
    "\n",
    "<center>![](./backprop-sketch/hausaufgabe.png)</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/1.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/2.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/3.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/4.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/5.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/6.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/7.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/8.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/9.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/10.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/11.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/12.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/13.gif>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>![](./backprop-sketch/14.gif)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "code_folding": [
     0,
     10,
     14,
     24,
     50,
     69,
     124,
     199
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120b2648588>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN AUC=1.000\n"
     ]
    }
   ],
   "source": [
    "# Einfaches neuronales Netz zur binären Klassifikation\n",
    "import random\n",
    "import math\n",
    "import time as t\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def f(x):\n",
    "    return math.tanh(x)\n",
    "\n",
    "\n",
    "def df(x):\n",
    "    try:\n",
    "        return 1 / (math.cosh(x) ** 2)\n",
    "    except OverflowError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "class Network:\n",
    "    \n",
    "\n",
    "    def __init__(self, l_in, l_hid, l_out, rate=0.05):\n",
    "        self.num_layer_in = l_in\n",
    "        self.num_layer_hid = l_hid\n",
    "        self.num_layer_out = l_out\n",
    "\n",
    "        self.w1 = []\n",
    "        self.w2 = []\n",
    "\n",
    "        # init weights inputs -> hidden\n",
    "        for i in range(self.num_layer_in * self.num_layer_hid ):\n",
    "            self.w1.append(random.randrange(-10, 10) * 0.01)\n",
    "\n",
    "        # init weights hidden -> output\n",
    "        for i in range(self.num_layer_hid * self.num_layer_out):\n",
    "            self.w2.append(random.randrange(-10, 10) * 0.01)\n",
    "\n",
    "        # init weights bias -> hidden\n",
    "        for i in range(self.num_layer_hid):\n",
    "            self.w1.append(random.randrange(-10, 10) * 0.01)\n",
    "\n",
    "        # init weights bias -> output\n",
    "        for i in range(self.num_layer_out):\n",
    "            self.w2.append(random.randrange(-10, 10) * 0.01)\n",
    "\n",
    "        self.learning_rate = rate\n",
    "\n",
    "    def forward_propagation(self, data_in):\n",
    "        net_hid = 0\n",
    "        out_hid = []\n",
    "        net_out = 0\n",
    "\n",
    "        # Calculate net input and outputs for hidden layer\n",
    "        for i in range(self.num_layer_hid):\n",
    "            net_hid += float(data_in[0]) * self.w1[i]\n",
    "            net_hid += float(data_in[1]) * self.w1[i + self.num_layer_hid]\n",
    "            net_hid += self.w1[i + self.num_layer_in * self.num_layer_hid]\n",
    "            out_hid.append(f(net_hid))\n",
    "\n",
    "        # Calculate net input and output for output layer\n",
    "        for i in range(self.num_layer_hid):\n",
    "            net_out += out_hid[i] * self.w2[i]\n",
    "        net_out += self.w2[self.num_layer_hid * self.num_layer_out]\n",
    "\n",
    "        return f(net_out)\n",
    "\n",
    "    def train(self, train_data):\n",
    "        for data in train_data:\n",
    "            net_hid = []\n",
    "            out_hid = []\n",
    "            net_out = 0\n",
    "            out_out = 0\n",
    "\n",
    "            updates_w1 = []\n",
    "            updates_w2 = []\n",
    "\n",
    "            # Calculate net input and outputs for hidden layer\n",
    "            net = 0\n",
    "            for i in range(self.num_layer_hid):\n",
    "                net += float(data[0]) * self.w1[i]\n",
    "                net += float(data[1]) * self.w1[i + self.num_layer_hid]\n",
    "                net += self.w1[i + self.num_layer_in * self.num_layer_hid]\n",
    "                net_hid.append(net)\n",
    "                out_hid.append(f(net))\n",
    "\n",
    "            # Calculate net input and output for output layer\n",
    "            net = 0\n",
    "            for i in range(self.num_layer_hid):\n",
    "                net += out_hid[i] * self.w2[i]\n",
    "            net += self.w2[self.num_layer_hid * self.num_layer_out]\n",
    "            net_out = net\n",
    "            out_out = f(net)\n",
    "\n",
    "            # w2 weights update\n",
    "            for i in range(len(self.w2)):\n",
    "                if i < len(self.w2) - self.num_layer_out:\n",
    "                    update = self.learning_rate * (float(data[2]) - out_out) * df(net_out) * out_hid[i]\n",
    "                else:\n",
    "                    update = self.learning_rate * (float(data[2]) - out_out) * df(net_out)\n",
    "                updates_w2.append(update)\n",
    "\n",
    "            # w1 weights update\n",
    "            for i in range(len(self.w1)):\n",
    "                if i < len(self.w1) - self.num_layer_hid:\n",
    "                    if i < 4:\n",
    "                        update = self.learning_rate * (float(data[2]) - out_out) * df(net_out) * self.w2[i % 4] * df(net_hid[i % 4]) * float(data[0])\n",
    "                    else:\n",
    "                        update = self.learning_rate * (float(data[2]) - out_out) * df(net_out) * self.w2[i % 4] * df(net_hid[i % 4]) * float(data[1])\n",
    "                else:\n",
    "                    update = self.learning_rate * (float(data[2]) - out_out) * df(net_out) * self.w2[i % 4] * df(net_hid[i % 4])\n",
    "                updates_w1.append(update)\n",
    "\n",
    "            # update weights w1\n",
    "            for i, update in enumerate(updates_w1):\n",
    "                self.w1[i] += update\n",
    "\n",
    "            # update weights w2\n",
    "            for i, update in enumerate(updates_w2):\n",
    "                self.w2[i] += update\n",
    "\n",
    "\n",
    "def main():\n",
    "    network = Network(2, 5, 1)\n",
    "    train_data = []\n",
    "    f = open('./input_dataset.in')\n",
    "\n",
    "    try:\n",
    "\n",
    "        while 1: \n",
    "            new_in = f.readline().split(',')\n",
    "            if len(new_in) < 3:\n",
    "                break\n",
    "            train_data.append(new_in)\n",
    "            \n",
    "\n",
    "        for _ in range(100):\n",
    "            network.train(train_data)\n",
    "\n",
    "        f = open('./testing_dataset.in')\n",
    "        out_data = []\n",
    "        test_data = []\n",
    "        out_data_prob = []\n",
    "        while 1:\n",
    "            new_in = f.readline().split(',')\n",
    "            if len(new_in) < 2:\n",
    "                break\n",
    "            new_in[0] = new_in[0][new_in[0].rfind(' ') + 1:]\n",
    "            new_in[1] = new_in[1][new_in[1].rfind(' ') + 1:]\n",
    "            output = network.forward_propagation(new_in)\n",
    "            test_data.append(new_in)\n",
    "            out_data_prob.append(output)\n",
    "            if output > 0:\n",
    "                output = '+1'\n",
    "            else:\n",
    "                output = '-1'\n",
    "            out_data.append(output)\n",
    "                \n",
    "        f = open('./expected_dataset.in')\n",
    "        out_expect_data = []\n",
    "        while 1:\n",
    "            new_in = f.readline().split('\\n')\n",
    "            if len(new_in) < 2:\n",
    "                break\n",
    "            out_expect_data.append(new_in[0])    \n",
    "\n",
    "    except EOFError:\n",
    "        print('', end='')\n",
    "        \n",
    "    plt.figure(3)\n",
    "    plt.plot(out_data, 'bo') \n",
    "    plt.plot(out_expect_data,'r+')\n",
    "    plt.title('Klassenlabels')\n",
    "    plt.box(\"off\")\n",
    "    plt.savefig('output-klass.png')\n",
    "    plt.show()\n",
    "\n",
    "    # AUC / ROC curves \n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from matplotlib import pyplot\n",
    "    lr_auc = roc_auc_score(list(map(int, out_expect_data)), out_data_prob)\n",
    "    # summarize scores\n",
    "    print('NN AUC=%.3f' % (lr_auc))\n",
    "    # calculate roc curves\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(list(map(int, out_expect_data)), out_data_prob)\n",
    "    # plot the roc curve for the model\n",
    "    pyplot.plot(lr_fpr, lr_tpr, marker='.', label='ANN')\n",
    "    # axis labels\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    # show the legend\n",
    "    pyplot.legend()\n",
    "    pyplot.savefig('output-roc.png')\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Analyse des Gradientenabstiegs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Abhängig von der Datenmenge machen wir einen **Kompromiss zwischen die Genauigkeit der Parameteraktualisierung und der Konvergenzgeschwindigkeit** [2, 4]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Analyse des Gradientenabstiegs : Genauigkeit\n",
    "\n",
    "<center>![](./precision.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Analyse des Gradientenabstiegs : Konvergenzgeschwindigkeit\n",
    "\n",
    "<center>![](./gradient-problems.gif)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Analyse des Gradientenabstiegs : Konvergenzgeschwindigkeit\n",
    "<center>![](./demo-anim.gif)</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fazit\n",
    "\n",
    "- **Gradientenabstieg** ist die **typische Optimierungsmethode** in neuronalen Netzen.\n",
    "- **Lernen** in Neuronalen Netzen ist ein **iterativer Prozess**.\n",
    "- Bei der **Backpropagation** wird ein Gradientenabstieg verwendet, um zu einer **Lösung zu konvergieren** (d. h. die **Gewichte zu finden**), die die **Fehlerfunktion minimiert**.\n",
    "- **Gradientenabstieg** ist jedoch **problematisch** (Konvergenz, Präzision), aber es wurden viele **verbesserte Verfahren** entwickelt.\n",
    "- Das **Verständnis des Gradientenabstieg** bei der Diagnose der Backpropagation **ist für erfolgreiche Anwendungen erforderlich**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Literaturverzeichnis\n",
    "\n",
    "[1] https://google-developers.appspot.com/machine-learning/ (letzter Besuch, Dez 2019)\n",
    "\n",
    "[2] Boyd, S., & Vandenberghe, L. (2004). Convex optimization. Cambridge university press.\n",
    "\n",
    "[3] Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press.\n",
    "\n",
    "[4] Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vorlesung Notebook herunterladen\n",
    "\n",
    "<img src=\"./qrcode.png\" style=\"width = 200, height=200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Nachteile des Gradientenabstiegs\n",
    "\n",
    "Es wurden **verschiedene Methoden entwickelt**, um die Nechteile zum **beheben** [3]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "| Gradient Descent | Nesterov Momentum |  Adaptive Moment Estimation (Adam) | \n",
    "|:---------:|:---------:|:---------:|\n",
    "| $ w_{t+1} = w_t - {\\alpha} {\\nabla E(w_t)}$   |  $ w_{t+1} = w_t - {\\alpha} {v_t} \\\\ { v_{t+1} = \\frac{\\gamma}{\\alpha} v_t  + {\\nabla E(w_t - \\gamma v_t)}} $  | $ w_{t+1} = w_t -{\\alpha} {\\frac{m_t}{\\sqrt{{v_t}}}}  \\\\ {m_{t+1} = f(\\nabla E(w_t) ) \\\\ v_{t+1} = f(\\nabla E(w_t)^2) }$| \n",
    "| $\\color{green}{+} $ einfache Aktualisierungsregel (z. B. Summe, Produkt)  | $\\color{green}{+}$ Beschleunigt in die jeweilige Steigungsrichtung und dämpft Schwingungen | $\\color{green}{+}$ speichert einen exponentiell abfallenden Durchschnitt vergangener Gradienten (glättendes Moment) mit adaptiver Lernrate für jeden parameter |\n",
    "| $\\color{red} {-}$ Konvergiert zum globalen Minimum für konvexe Fehleroberflächen und zu einem lokalen Minimum für nichtkonvexe Oberflächen | $\\color{red}{-}$ funktioniert nicht gut, wenn die Kostenfunktionen stark konvex sind | $\\color{red}{-}$ hat den niedrigsten Trainingsfehler, aber nicht den niedrigsten Validierungsfehler, und der Validierungsfehler ist größer als der Trainingsfehler (d. h. leichte Überanpassung) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Analyse\n",
    "<center>![](./loss-anim.gif)</center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
